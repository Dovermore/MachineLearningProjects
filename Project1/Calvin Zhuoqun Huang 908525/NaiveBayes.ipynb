{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Zhuoqun (Calvin) Huang\n",
    "###### Python version: Python 3.6\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for loading the csv files\n",
    "import csv\n",
    "import os\n",
    "import os.path as path\n",
    "# Possible exploration of data through visualisation\n",
    "import seaborn as sns\n",
    "import sys\n",
    "# pandas for it's an easier way to reference rows and columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "import abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup global variables for automated processing\n",
    "base_path = path.abspath(\"./data\")\n",
    "data_files = [\n",
    "    \"anneal.csv\", \"breast-cancer.csv\", \"car.csv\", \"cmc.csv\", \"hepatitis.csv\",\n",
    "    \"hypothyroid.csv\", \"mushroom.csv\", \"nursery.csv\", \"primary-tumor.csv\"\n",
    "]\n",
    "headers_file = \"headers.txt\"\n",
    "data_path = [path.join(base_path, file) for file in data_files]\n",
    "headers_path = path.join(base_path, headers_file)\n",
    "\n",
    "\n",
    "def to_array(X):\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X = X.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit_transform(X, y=None):\n",
    "        pass\n",
    "\n",
    "class BasePredictor(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(X, y=None):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def predict(X, y=None):\n",
    "        pass\n",
    "    \n",
    "class BaseScorer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def score(X, y, metrics=\"accuracy\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestSplit(BaseTransformer):\n",
    "    \"\"\"\n",
    "    Class for splitting the data to train/test set\n",
    "    \"\"\"\n",
    "    def __init__(self, ratio=0.2):\n",
    "        \"\"\"\n",
    "        Initialise a TrainTestSplit class for splitting the data to train/test split\n",
    "        \n",
    "        Args:\n",
    "            ratio (float): Ratio of which the test set will have\n",
    "        \"\"\"\n",
    "        self._ratio = ratio\n",
    "        \n",
    "    def fit_transform(self, X, y=None, seed=1234):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            m (int): The number to shuffle\n",
    "        \"\"\"\n",
    "        X = to_array(X)\n",
    "        if y is not None:\n",
    "            y = to_array(y)\n",
    "        \n",
    "        n, k = X.shape\n",
    "        random.seed(seed)\n",
    "        indexes = list(range(n))\n",
    "        shuffle(indexes)\n",
    "        mid = math.ceil(n * self._ratio)\n",
    "        test_ind = indexes[:mid]\n",
    "        train_ind = indexes[mid:]\n",
    "        \n",
    "        if y is not None:\n",
    "            return X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "        return X[test_ind], X[train_ind]\n",
    "    \n",
    "class KFoldValidation(BaseTransformer):\n",
    "    \"\"\"\n",
    "    Performs K-fold validation for given model\n",
    "    \"\"\"\n",
    "    def __init__(self, predictor, scorer, k=10):\n",
    "        \"\"\"\n",
    "        Use the given scorer to compute a K fold validation\n",
    "        \n",
    "        Args:\n",
    "            scorer: instance of BaseScorer, to score the prediction\n",
    "            k (int): number of folds\n",
    "        \"\"\"\n",
    "        \n",
    "        # I'm too lazy to write a new error class\n",
    "        assert isinstance(predictor, BasePredictor) and isinstance(scorer, BaseScorer)\n",
    "        self._predictor = predictor\n",
    "        self._scorer = scorer\n",
    "        self._k = k\n",
    "        \n",
    "    def fit_transform(self, X, y, avg=False, shuf=True, seed=1234):\n",
    "        \"\"\"\n",
    "        K fold validate the predictor and return all k fold evaluations\n",
    "        \"\"\"\n",
    "        X = to_array(X)\n",
    "        if y is not None:\n",
    "            y = to_array(y)\n",
    "\n",
    "        n, k = X.shape\n",
    "        \n",
    "\n",
    "        indexes = list(range(n))\n",
    "        if shuf:\n",
    "            random.seed(seed)\n",
    "            shuffle(indexes)\n",
    "        # Double length of thie index list\n",
    "        indexes *= 2\n",
    "        interval = math.ceil(n / self._k)\n",
    "        lo, hi = 0, interval\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(self._k):\n",
    "             \n",
    "            train_index = indexes[:lo] + indexes[hi:]\n",
    "            test_index = indexes[lo:hi]\n",
    "            \n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            \n",
    "            self._predictor.fit(X_train, y_train)\n",
    "            y_hat = self._predictor.predict(X_test)\n",
    "            \n",
    "            scores.append(self._scorer.score(y_test, y_hat))\n",
    "            \n",
    "            lo += interval\n",
    "            hi += interval\n",
    "\n",
    "        if avg:\n",
    "            return np.average(scores)\n",
    "        return np.array(scores)\n",
    "    \n",
    "class AccuracyScorer(BaseScorer):\n",
    "    \"\"\"\n",
    "    I think this code is easy enough for anyone to understand\n",
    "    and the name is self explanatory\n",
    "    \"\"\"\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Array of labels\n",
    "            y: Array of labels\n",
    "        \"\"\"\n",
    "        X = to_array(X)\n",
    "        y = to_array(y)\n",
    "        \n",
    "        assert X.shape == y.shape\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        acc = 0\n",
    "        for i in range(n):\n",
    "            acc += 1/n if X[i] == y[i] else 0\n",
    "        return acc\n",
    "    \n",
    "class dd_float():\n",
    "    \"\"\"\n",
    "    float type defaultdict factory of default dictionary \n",
    "    \n",
    "    Save the hassle of initialising a dictionary\n",
    "    \"\"\"\n",
    "    def __new__(self):\n",
    "        return dd(float)\n",
    "    \n",
    "class dd_special():\n",
    "    \"\"\"\n",
    "    final special type of factory\n",
    "    \n",
    "    for initialising (defaultdict of (defaultdict of float))\n",
    "    \"\"\"\n",
    "    def __new__(self):\n",
    "        return dd(dd_float)\n",
    "    \n",
    "class NotFittedError(BaseException):\n",
    "    def __init__(self, msg=None):\n",
    "        if msg:\n",
    "            print(msg, file=sys.stderr)\n",
    "        else:\n",
    "            print(\"This model is not yet fitted!\", file=sys.stderr)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNB(BasePredictor, BaseScorer):\n",
    "    \"\"\"\n",
    "    SimpleNB defines a very simple categorial naive bayes classifier\n",
    "    \n",
    "    Attributes:\n",
    "        attribute_information_gain: After the model is fitted, one can obtain the\n",
    "                information gain of attribute value wrt class label\n",
    "        class_information_gain: After the model is fitted, one can obtain the\n",
    "                information gain of class label wrt attribute value (duplicate)\n",
    "        _labels: a dictionary mapping each index to a corresponding label. the same order as\n",
    "                the y vector used to train the model\n",
    "        _hyper_params: get the hyper parameter set of this model\n",
    "        _params: get the parameter set of this model\n",
    "    \n",
    "    Methods:\n",
    "        __init__(smoothing={\"epsilon\", float}): Initialise the mode with specific smoothing method,\n",
    "                default to laplace smoothing with 1 as smoothing factor.\n",
    "        fit(X, y): Fit the model for given data.\n",
    "        predict_proba(X, y=None): Predict and returns the probability of each outcome (after applying\n",
    "                normalising factor). Raise Error if model is not yet fitted\n",
    "        predict(X, y=None): predict the new data based on trained model. Will raise Error if model\n",
    "                is not yet fitted\n",
    "        score(X, y): Returns accuracy score\n",
    "        evaluate(X, y, metrics={\"accuracy\", \"f1_score\", \"precision\", \"recall\", \"entropy\", \"matrix\"}): evaluate \n",
    "                the model. WARNING: will return different data structures. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=1.0, epsilon=1e-12):\n",
    "        \"\"\"\n",
    "        Initialise a new Naive Bayes Classifier\n",
    "        \n",
    "        Args:\n",
    "            smoothing: Speficies the smoothing method to use\n",
    "            epsilon: The epsilon factor to use\n",
    "        \"\"\"\n",
    "        self._smoothing = smoothing\n",
    "        \n",
    "        # Initialise the dictionaries\n",
    "        self._prior = dd(float)\n",
    "        self._conditional = dd(dd_special)\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "        self._accuracy_scorer = AccuracyScorer()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model based on give X and y\n",
    "        \n",
    "        Args:\n",
    "            X (2-d np.array): The instance and attributes\n",
    "            y (1-d np.array): The corresponding labels\n",
    "        \"\"\"\n",
    "        # Convert to np.array if pandas data frame are fed in\n",
    "        X = to_array(X)\n",
    "        y = to_array(y)\n",
    "            \n",
    "        self._X = X\n",
    "        self._y = y\n",
    "            \n",
    "        # Counts the occurance of each label: (label) -> (count)\n",
    "        self._label_counts = dd(float)      \n",
    "        # Counts the unique labels (index) -> (label)\n",
    "        self._labels = {}\n",
    "        # Counts the conditional counts (of an attribute column) \n",
    "        # of (a specifial attribute value given a class label)\n",
    "        # keys:(attribute, label, value)-> (count)\n",
    "        self._conditional_counts = dd(dd_special)\n",
    "        # Coutns unique attribute values of each attribute: (attribute) -> (set(values))\n",
    "        self._values = dd(set)\n",
    "        \n",
    "        # Count up all instances\n",
    "        n, k = X.shape\n",
    "        \n",
    "        # update label counts\n",
    "        i = 0 # to construct the dictionary\n",
    "        for row in range(n):\n",
    "            label = y[row]\n",
    "            self._label_counts[label] += 1.0\n",
    "            if label not in self._labels.values():\n",
    "                # Add to the dictionary and advance the counter\n",
    "                self._labels[i] = label\n",
    "                i += 1\n",
    "        \n",
    "        for col in range(k):\n",
    "            attri_dd = self._conditional_counts[col]\n",
    "            value_dd = self._values[col]\n",
    "            for row in range(n):\n",
    "                label = y[row]\n",
    "                value = X[row][col]\n",
    "                # if the value is not a NaN\n",
    "                if not pd.isnull(value):\n",
    "                    # Update the number of unique values\n",
    "                    value_dd.add(value)\n",
    "                    # Update conditional counts\n",
    "                    attri_dd[label][value] += 1.0\n",
    "\n",
    "        # Build the prior dictionary\n",
    "        self.__build_prior()\n",
    "        self.__build_conditional()\n",
    "        \n",
    "    def predict_proba(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Return an array of probabilities of being of being each of the classes,\n",
    "        the order of which it's indexed is the same as in training phase (The order\n",
    "        which at which they are observed).\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        proba_array = []\n",
    "        \n",
    "        X = to_array(X)\n",
    "        \n",
    "        # Number of instances and number of attributes\n",
    "        n, k = X.shape\n",
    "\n",
    "        for row in range(n):\n",
    "            # Load prior from dictionary in order\n",
    "            row_proba = [self._prior[label] for (ind, label)\n",
    "                         in sorted(self._labels.items(), key=lambda x: x[0])]\n",
    "            # Iterate through all columns\n",
    "            for col in range(k):\n",
    "                value = X[row][col]\n",
    "\n",
    "                # Skip null value or unseen values\n",
    "                if pd.isnull(value) or value not in self._values[col]:\n",
    "                    continue\n",
    "                    \n",
    "                # Iterate through all classes\n",
    "                for ind, label in self._labels.items():\n",
    "                    row_proba[ind] *= self._conditional[col][label][value]\n",
    "            # get the total probability\n",
    "            total_proba = sum(row_proba)\n",
    "        \n",
    "            # Normalise by total_proba\n",
    "            row_proba = [i/total_proba for i in row_proba]\n",
    "            proba_array.append(row_proba)\n",
    "\n",
    "        return np.array(proba_array)\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        proba_array = self.predict_proba(X)\n",
    "        predictions = []\n",
    "        for row in proba_array:\n",
    "            index = self.argmax(row)\n",
    "            predictions.append(self._labels[index])\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def score(self, X=None, y=None, metrics=\"accuracy\"):\n",
    "        \"\"\"\n",
    "        Scores own performance based on provided data (if no data then on training data)\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        \n",
    "        if X is None:\n",
    "            X = self._X\n",
    "            y = self._y\n",
    "        y_hat = self.predict(X)\n",
    "        return self._accuracy_scorer.score(y, y_hat)\n",
    "                \n",
    "    def __build_prior(self):\n",
    "        \"\"\"\n",
    "        Build the prior dictionary based on collected values\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        self._prior = {}\n",
    "\n",
    "        labels = self._labels.values()\n",
    "        # Count up all instances\n",
    "        n, k = self._X.shape\n",
    "        n = float(n)\n",
    "        \n",
    "        for label in labels:\n",
    "            self._prior[label] = self._label_counts[label] / n\n",
    "    \n",
    "    def __build_conditional(self):\n",
    "        \"\"\"\n",
    "        Build the conditional probability dictionary based on collected values,\n",
    "        and with the given smoothing method.\n",
    "        \n",
    "        This method builds a compelete dictionary that indexes possible (value, label) pair\n",
    "        in the training set. No need for further processing\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        \n",
    "        # clear what's in the dictionary from last fitting process\n",
    "        self._conditional = dd(dd_special)\n",
    "        \n",
    "        # Get all possible labels\n",
    "        labels = self._labels.values()\n",
    "        # Get all possible columns\n",
    "        cols = self._values.keys()\n",
    "        \n",
    "        proba_fn = self.__laplace if isinstance(self._smoothing, float) else self.__epsilon\n",
    "        \n",
    "        for col in cols:\n",
    "            values = self._values[col]\n",
    "            for label in labels:\n",
    "                for value in values:\n",
    "                    self._conditional[col][label][value] = \\\n",
    "                        proba_fn(col, label, value, V=len(values))\n",
    "                \n",
    "    def __laplace(self, col, label, value, **kwargs):\n",
    "        \"\"\"\n",
    "        Function for laplace smoothing\n",
    "        \"\"\"\n",
    "        V = kwargs[\"V\"]\n",
    "        return (\n",
    "            (self._smoothing + self._conditional_counts[col].get(label, {}).get(value, 0)) /\n",
    "            (V + self._label_counts[label])   \n",
    "               )\n",
    "    \n",
    "    def __epsilon(self, col, label, value, **kwargs):\n",
    "        \"\"\"\n",
    "        Function for epsilon smoothing\n",
    "        \"\"\"\n",
    "        return self._conditional_counts[col].get(label, {})\\\n",
    "                .get(value, self._epsilon) / self._label_counts[label]\n",
    "\n",
    "    @staticmethod\n",
    "    def __count_total(dictionary):\n",
    "        \"\"\"\n",
    "        Counts the total number of item in a dictionary\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        if isinstance(dictionary, dict):\n",
    "            for key in dictionary:\n",
    "                total += dictionary[key]\n",
    "        return total\n",
    "    \n",
    "    @staticmethod\n",
    "    def argmax(l, val=False):\n",
    "        \"\"\"\n",
    "        Simply get the max value out of the list,\n",
    "        val specifies if the max value should also be returned\n",
    "        \"\"\"\n",
    "        vm, im = -math.inf, 0\n",
    "        for i, v in enumerate(l):\n",
    "            if v > vm:\n",
    "                im = i\n",
    "                vm = v\n",
    "        if val:\n",
    "            return im, vm\n",
    "        return im\n",
    "            \n",
    "    def __check_fitted(self):\n",
    "        \"\"\"\n",
    "        Check if the model is fitted, if not will raise NotFittedError\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._X\n",
    "        except NameError:\n",
    "            raise NotFittedError(\"This NB model is not fitted yet, please fit it first\")\n",
    "        \n",
    "    @property\n",
    "    def _hyper_params(self):\n",
    "        return {\n",
    "            \"_smoothing\": self._smoothing,\n",
    "            \"_epsilon\": self._epsilon\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def _params(self):\n",
    "        \"\"\"\n",
    "        All parameter returned as nested defaultdicts\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"_priors\" : self._prior,\n",
    "            \"conditionals\" : self._conditional\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def class_information_gains(self):\n",
    "        \"\"\"\n",
    "        stores entropy of each attributes\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        \n",
    "        cols = self._values.keys()\n",
    "        labels = self._labels.keys()\n",
    "        \n",
    "        label_entropy = self.__entropy(list(self._prior.values()))\n",
    "        \n",
    "        conditional_entropy = 0\n",
    "        information_gains = {}\n",
    "        attribute_probas = self._attribute_probas\n",
    "        for col in cols:\n",
    "            attribute_proba_dict = attribute_probas[col]\n",
    "            # posterior_dict = self.__invert_proba(self._conditional[col], self._prior)\n",
    "            posterior_dict = self.__invert_proba2(self._conditional_counts[col])\n",
    "            for value, probas_dict in posterior_dict.items():\n",
    "                conditional_entropy += self.__entropy(list(probas_dict.values()))\\\n",
    "                        * attribute_proba_dict[value]\n",
    "            information_gains[col] = label_entropy - conditional_entropy\n",
    "            conditional_entropy = 0\n",
    "        return information_gains\n",
    "    \n",
    "    @property\n",
    "    def _attribute_probas(self):\n",
    "        \"\"\"\n",
    "        Stores all attribute values distribution\n",
    "        \"\"\"\n",
    "        full_proba_dict = {}\n",
    "        for col in self._conditional_counts:\n",
    "            conditional_count_dict = self._conditional_counts[col]\n",
    "            proba_dict = dd(float)\n",
    "\n",
    "            for label, counts_dict in conditional_count_dict.items():\n",
    "                for value, counts in counts_dict.items():\n",
    "                    proba_dict[value] += counts\n",
    "            total = sum(proba_dict.values())\n",
    "            proba_dict = {i: proba_dict[i]/total for i in proba_dict.keys()}\n",
    "            full_proba_dict[col] = proba_dict\n",
    "        return full_proba_dict\n",
    "                \n",
    "    @staticmethod\n",
    "    def __invert_proba(conditional_dict, prior_dict):\n",
    "        \"\"\"\n",
    "        Invert the nested (label) -> (value) -> (proba) structure or\n",
    "        in other words probability p(value|label) to p(label|value)\n",
    "        by computing p(label|value) = k * p(value|label) * p(label)\n",
    "        \"\"\"\n",
    "        posterior_dict = dd(dd_float)\n",
    "        for label, probas_dict in conditional_dict.items():\n",
    "            for value, proba in probas_dict.items():\n",
    "                posterior_dict[value][label] = proba * prior_dict.get(label, 0)\n",
    "        # Normalise the dict\n",
    "        for value, probas_dict in posterior_dict.items():\n",
    "            total = sum(probas_dict.values())\n",
    "            for label, proba in probas_dict.items():\n",
    "                probas_dict[label] = proba / total\n",
    "        return posterior_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def __invert_proba2(conditional_count_dict):\n",
    "        \"\"\"\n",
    "        Invert the (label) -> (value) -> (proba). Same as __invert_proba,\n",
    "        but using count instead of probas\n",
    "        \"\"\"\n",
    "        invert_dict = dd(dd_float)\n",
    "        # Count up the inverse\n",
    "        for label, count_dict in conditional_count_dict.items():\n",
    "            for value, count in count_dict.items():\n",
    "                invert_dict[value][label] += count\n",
    "        # Turn into probabilities\n",
    "        for value, count_dict in invert_dict.items():\n",
    "            total = sum(count_dict.values())\n",
    "            invert_dict[value] = {label:count/total for label, count in \n",
    "                                  invert_dict[value].items()}\n",
    "        return invert_dict\n",
    "                \n",
    "    @property\n",
    "    def attribute_information_gains(self):\n",
    "        \"\"\"\n",
    "        stores information gain for attribute wrt to classes\n",
    "        \"\"\"\n",
    "        information_gains = {}\n",
    "\n",
    "        attribute_probas = self._attribute_probas\n",
    "        # Prior is just label probabilities\n",
    "        label_probas = self._prior\n",
    "        \n",
    "        for col, label_count_dict in self._conditional_counts.items():\n",
    "            \n",
    "            attribute_proba = attribute_probas[col]\n",
    "            attribute_entropy = self.__entropy(attribute_proba.values())\n",
    "            conditional_entropy = 0\n",
    "            \n",
    "            for label, count_dict in label_count_dict.items():\n",
    "                entropy = self.__entropy(count_dict.values())\n",
    "                conditional_entropy += entropy * self._prior[label]\n",
    "                \n",
    "            information_gains[col] = attribute_entropy - conditional_entropy\n",
    "        return information_gains\n",
    "            \n",
    "    @staticmethod\n",
    "    def __entropy(probs):\n",
    "        \"\"\"\n",
    "        Straight forward entropy calculation\n",
    "        \"\"\"\n",
    "        # Check it's indeed probability\n",
    "        total = sum(probs)\n",
    "        # Normalise the probability (This should be used instead of checking,\n",
    "        # for debugging purpose, enable the assert for now)\n",
    "        probs = [i/total for i in probs]\n",
    "        assert abs(sum(probs) - 1) < 1e-7\n",
    "        entropies = [-p*np.log2(p) for p in probs]\n",
    "        return sum(entropies)\n",
    "    \n",
    "    @property\n",
    "    def summary(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        If no data given, then use training data\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        summary = {}\n",
    "        if X is None:\n",
    "            X = self._X\n",
    "            y = self._y\n",
    "        score = self.score(X, y)\n",
    "        \n",
    "        ig = self.attribute_information_gains\n",
    "        total_ig = sum(ig.values())\n",
    "        avg_ig = total_ig / len(ig.values())\n",
    "        ig[\"max_ig\"] = max(ig.values())\n",
    "        ig[\"total_ig\"] = total_ig\n",
    "        ig[\"avg_ig\"] = avg_ig\n",
    "\n",
    "        summary = {\"score\": score}\n",
    "        summary.update(ig)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(data_file, base_path=base_path, headers=headers_file, split=False):\n",
    "    \"\"\"\n",
    "    Load the data and return a data frame of data back\n",
    "    \n",
    "    Args:\n",
    "        data_file: The name of file (without path)\n",
    "        base_path: Base path to data and header\n",
    "        headers: The name of headers file (Set to None to prevent loading header)\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame of all instances\n",
    "    \n",
    "        pd.DataFrame: all instance and attributes\n",
    "        pd.Series: all labels\n",
    "    \"\"\"\n",
    "    # define full path\n",
    "    full_data_file = path.join(base_path, data_file)\n",
    "    full_headers_file = path.join(base_path, headers_file)\n",
    "    \n",
    "    # Load dataframe\n",
    "    data_df = pd.read_csv(full_data_file, header=None, na_values=\"?\")\n",
    "    \n",
    "    # Load header file\n",
    "    row = None\n",
    "    if headers:\n",
    "        with open(full_headers_file) as headerf:\n",
    "            lines = headerf.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                if data_file in line:\n",
    "                    row = lines[i+2] # Set it to the correct rows\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"The header of {data_file} is absent in {headers}\")\n",
    "        # If found the corresponding row, load to dataframe\n",
    "    if row:\n",
    "        columns = row.split(\",\")\n",
    "        data_df.columns = columns\n",
    "    if split:\n",
    "        return data_df.iloc[:, :-1], data_df.iloc[:, -1]\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(X, y, predictor):\n",
    "    assert isinstance(predictor, BasePredictor)\n",
    "    predictor.fit(X, y)\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(X, predictor):\n",
    "    assert isinstance(predictor, BasePredictor)\n",
    "    y_hat = predictor.predict(X)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(y_hat, y, scorer):\n",
    "    assert isinstance(scorer, BaseScorer)\n",
    "    return scorer.score(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(simple_nb):\n",
    "    assert isinstance(simple_nb, SimpleNB)\n",
    "    return simple_nb.attribute_information_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y, y_hat):\n",
    "    unqiue_values = set(y) | set(y_hat)\n",
    "    # map class to index\n",
    "    mapping = {j: i for i, j in enumerate(unqiue_values)}\n",
    "    order = sorted(mapping.keys(), key=lambda x: mapping[x])\n",
    "    matrix = pd.DataFrame(0, columns=order, index=order)\n",
    "    for ind, label in enumerate(y):\n",
    "        predicted = y_hat[ind]\n",
    "        matrix.loc[predicted, label] += 1\n",
    "    for ind, data in matrix.iterrows():\n",
    "        matrix.loc[data.name, \"total\"] = sum(data)\n",
    "    for col in matrix:\n",
    "        column = matrix[col]\n",
    "        matrix.loc[\"total\", column.name] = column.sum()\n",
    "    return matrix\n",
    "\n",
    "def make_confusion_matrix(data_file, split=False, *args, **kwargs):\n",
    "    X, y = preprocess(data_file, split=True)\n",
    "\n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = splitter.fit_transform(X, y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = X, X, y, y\n",
    "    train(X_train, y_train, simple_nb)\n",
    "    y_hat = predict(X_test, simple_nb)\n",
    "    return confusion_matrix(y_test, y_hat)\n",
    "    \n",
    "def show_null_portion(X):\n",
    "    n, k = X.shape\n",
    "    null_rates = []\n",
    "    for col in X:\n",
    "        c = 0\n",
    "        column = X[col]\n",
    "        for item in column:\n",
    "            if pd.isnull(item):\n",
    "                c += 1\n",
    "        null_rates.append(c/n)\n",
    "    return pd.DataFrame([null_rates], columns=X.columns, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_evaluation(split=0, data_file=None, baseline=False, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Prints accuracy with the information gain of each columns\n",
    "    \"\"\"\n",
    "    if baseline:\n",
    "        scores = pd.DataFrame(columns=[\"data\", \"baseline\", \"score\", \"total_ig\", \"max_ig\", \"avg_ig\"])\n",
    "    else:\n",
    "        scores = pd.DataFrame(columns=[\"data\", \"score\", \"total_ig\", \"max_ig\", \"avg_ig\"])\n",
    "\n",
    "    simple_nb = SimpleNB(*args, **kwargs)\n",
    "    splitter = TrainTestSplit(split)\n",
    "    scorer = AccuracyScorer()\n",
    "    \n",
    "    files = data_files\n",
    "    if data_file:\n",
    "        files = [data_file]        \n",
    "    for i, data_file in enumerate(files):\n",
    "        X, y = preprocess(data_file, split=True)\n",
    "        \n",
    "        if split:\n",
    "            X_train, X_test, y_train, y_test = splitter.fit_transform(X, y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = X, X, y, y\n",
    "        train(X_train, y_train, simple_nb)\n",
    "        row = simple_nb.summary\n",
    "        row[\"data\"] = data_file\n",
    "        if baseline:\n",
    "            row[\"baseline\"] = sorted(simple_nb._params[\"_priors\"].values(), reverse=True)[0]\n",
    "            row[\"difference\"] = row[\"score\"] - row[\"baseline\"]\n",
    "        scores = scores.append(row, ignore_index=True)\n",
    "    scores = scores.set_index(\"data\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1 and Q4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Let's look at how accuracy compares to various statistics of information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_ig</th>\n",
       "      <th>max_ig</th>\n",
       "      <th>avg_ig</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anneal.csv</th>\n",
       "      <td>0.991091</td>\n",
       "      <td>3.087582</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.088217</td>\n",
       "      <td>0.409090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306052</td>\n",
       "      <td>0.051344</td>\n",
       "      <td>0.291082</td>\n",
       "      <td>0.147119</td>\n",
       "      <td>0.213723</td>\n",
       "      <td>0.292235</td>\n",
       "      <td>0.126166</td>\n",
       "      <td>0.141074</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.038702</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117225</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>0.137181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04324</td>\n",
       "      <td>0.033038</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast-cancer.csv</th>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.306648</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.025819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car.csv</th>\n",
       "      <td>0.873843</td>\n",
       "      <td>0.686494</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.114416</td>\n",
       "      <td>0.096449</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.219663</td>\n",
       "      <td>0.030008</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmc.csv</th>\n",
       "      <td>0.505771</td>\n",
       "      <td>0.303960</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.037995</td>\n",
       "      <td>0.070906</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis.csv</th>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.696271</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.053559</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.083971</td>\n",
       "      <td>0.081544</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.109644</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.084933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothyroid.csv</th>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.040715</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mushroom.csv</th>\n",
       "      <td>0.997169</td>\n",
       "      <td>4.444996</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.202045</td>\n",
       "      <td>0.048797</td>\n",
       "      <td>0.028590</td>\n",
       "      <td>0.036049</td>\n",
       "      <td>0.192379</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>0.230154</td>\n",
       "      <td>0.416978</td>\n",
       "      <td>0.007517</td>\n",
       "      <td>0.191740</td>\n",
       "      <td>0.284726</td>\n",
       "      <td>0.271894</td>\n",
       "      <td>0.253845</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.038453</td>\n",
       "      <td>0.318022</td>\n",
       "      <td>0.480705</td>\n",
       "      <td>0.201958</td>\n",
       "      <td>0.156834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery.csv</th>\n",
       "      <td>0.903086</td>\n",
       "      <td>1.291786</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.072935</td>\n",
       "      <td>0.196449</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary-tumor.csv</th>\n",
       "      <td>0.604720</td>\n",
       "      <td>3.315571</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.195034</td>\n",
       "      <td>0.154742</td>\n",
       "      <td>0.321966</td>\n",
       "      <td>0.382902</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.212462</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.220522</td>\n",
       "      <td>0.199761</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.053688</td>\n",
       "      <td>0.291530</td>\n",
       "      <td>0.127154</td>\n",
       "      <td>0.240324</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>0.170148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score  total_ig    max_ig    avg_ig         0         1  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.991091  3.087582  0.435178  0.088217  0.409090  0.000000   \n",
       "breast-cancer.csv  0.755245  0.306648  0.077010  0.034072  0.010606  0.002002   \n",
       "car.csv            0.873843  0.686494  0.262184  0.114416  0.096449  0.073704   \n",
       "cmc.csv            0.505771  0.303960  0.101740  0.037995  0.070906  0.040139   \n",
       "hepatitis.csv      0.851613  0.696271  0.132771  0.053559  0.036607  0.013136   \n",
       "hypothyroid.csv    0.952261  0.040715  0.009354  0.002262  0.000245  0.000914   \n",
       "mushroom.csv       0.997169  4.444996  0.906075  0.202045  0.048797  0.028590   \n",
       "nursery.csv        0.903086  1.291786  0.958775  0.161473  0.072935  0.196449   \n",
       "primary-tumor.csv  0.604720  3.315571  0.499848  0.195034  0.154742  0.321966   \n",
       "\n",
       "                          2         3         4         5         6         7  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.306052  0.051344  0.291082  0.147119  0.213723  0.292235   \n",
       "breast-cancer.csv  0.057171  0.068995  0.053458  0.077010  0.002489  0.009099   \n",
       "car.csv            0.004486  0.219663  0.030008  0.262184       NaN       NaN   \n",
       "cmc.csv            0.101740  0.009821  0.002582  0.030474  0.032511  0.015786   \n",
       "hepatitis.csv      0.014491  0.083971  0.081544  0.012010  0.008603  0.002339   \n",
       "hypothyroid.csv    0.001238  0.000148  0.000999  0.001368  0.000542  0.000435   \n",
       "mushroom.csv       0.036049  0.192379  0.906075  0.014165  0.100883  0.230154   \n",
       "nursery.csv        0.005573  0.011886  0.019602  0.004333  0.022233  0.958775   \n",
       "primary-tumor.csv  0.382902  0.499848  0.212462  0.020367  0.100881  0.067873   \n",
       "\n",
       "                          8         9        10        11        12        13  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.126166  0.141074  0.032488  0.435178  0.038702  0.000438   \n",
       "breast-cancer.csv  0.025819       NaN       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv      0.036702  0.109644  0.132771  0.079521  0.084933       NaN   \n",
       "hypothyroid.csv    0.000489  0.000898  0.000045  0.000079  0.009354  0.004075   \n",
       "mushroom.csv       0.416978  0.007517  0.191740  0.284726  0.271894  0.253845   \n",
       "nursery.csv             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.220522  0.199761  0.067145  0.053688  0.291530  0.127154   \n",
       "\n",
       "                         14        15        16        17        18        19  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.039356  0.021775  0.037997  0.036703  0.000000  0.117225   \n",
       "breast-cancer.csv       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hypothyroid.csv    0.005793  0.005768  0.005744  0.002580       NaN       NaN   \n",
       "mushroom.csv       0.241416  0.000000  0.023817  0.038453  0.318022  0.480705   \n",
       "nursery.csv             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.240324  0.184258  0.170148       NaN       NaN       NaN   \n",
       "\n",
       "                         20        21   22        23        24   25        26  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.029754  0.027042  0.0  0.015605  0.137181  0.0  0.022397   \n",
       "breast-cancer.csv       NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "car.csv                 NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "cmc.csv                 NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "hypothyroid.csv         NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "mushroom.csv       0.201958  0.156834  NaN       NaN       NaN  NaN       NaN   \n",
       "nursery.csv             NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "primary-tumor.csv       NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "\n",
       "                         27   28   29   30       31        32        33  \\\n",
       "data                                                                      \n",
       "anneal.csv         0.018242  0.0  0.0  0.0  0.04324  0.033038  0.019379   \n",
       "breast-cancer.csv       NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "car.csv                 NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "cmc.csv                 NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "hepatitis.csv           NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "hypothyroid.csv         NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "mushroom.csv            NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "nursery.csv             NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "primary-tumor.csv       NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "\n",
       "                         34  \n",
       "data                         \n",
       "anneal.csv         0.003959  \n",
       "breast-cancer.csv       NaN  \n",
       "car.csv                 NaN  \n",
       "cmc.csv                 NaN  \n",
       "hepatitis.csv           NaN  \n",
       "hypothyroid.csv         NaN  \n",
       "mushroom.csv            NaN  \n",
       "nursery.csv             NaN  \n",
       "primary-tumor.csv       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q1_table = q1_evaluation(split=0, smoothing=\"epsilon\", epsilon=1e-12, baseline=False)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(q1_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these outputs. It's obvious we got somewhat a **positive relations between score(accuracy) and the various metrics of information gain**. Namely, the **more overall/max per attribute/average information gain the better accuracy** the result will have.\n",
    "\n",
    "---\n",
    "---\n",
    "However, we do can observe two possible outliers and one interesting observation.\n",
    "1. The **breast-cancer.csv** file which has relative **good overall accuracy**, but extremely **low IG**.\n",
    "2. The **primary-tumor.csv** which has very **high information gain** but rather **low accuracy** of 60%\n",
    "---\n",
    "3. The **mushroom.csv** has a very high **single attribute information gain** while having rather **lower overall IG** and achieved a **very good accuracy**\n",
    "\n",
    "We perform some preliminary data analysis on those datasets.\n",
    "\n",
    "---\n",
    "First let's analyse the breast-cancer.csv case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no-recurrence-events</th>\n",
       "      <th>recurrence-events</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no-recurrence-events</th>\n",
       "      <td>170.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recurrence-events</th>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>201.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>286.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      no-recurrence-events  recurrence-events  total\n",
       "no-recurrence-events                 170.0               39.0  209.0\n",
       "recurrence-events                     31.0               46.0   77.0\n",
       "total                                201.0               85.0  286.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>menopause</th>\n",
       "      <th>tumor-size</th>\n",
       "      <th>inv-nodes</th>\n",
       "      <th>node-caps</th>\n",
       "      <th>deg-malig</th>\n",
       "      <th>breast</th>\n",
       "      <th>breast-quad</th>\n",
       "      <th>irradiat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  menopause  tumor-size  inv-nodes  node-caps  deg-malig  breast  \\\n",
       "0  0.0        0.0         0.0        0.0   0.027972        0.0     0.0   \n",
       "\n",
       "   breast-quad  irradiat  \n",
       "0     0.003497       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_nb = SimpleNB(\"epsilon\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(make_confusion_matrix(\"breast-cancer.csv\"))\n",
    "    # display(make_confusion_matrix(\"cmc.csv\"))\n",
    "    X, y = preprocess(\"breast-cancer.csv\", split=True)\n",
    "    # display(df) \n",
    "    # Very high null rate col 2 and 3\n",
    "    display(show_null_portion(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is rather clear from this analysis. We can see from this confusion matrix that the **majority of classes** are **no-recurrence-events**, totaling more than **73% of overall labels**. Because the **0-R benchmark** achieved this high accuracy, it no suprising that the model managed a reasonable accuracy despite very low information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, delve into the second outlier dataset. Here are some analysis on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U</th>\n",
       "      <th>F</th>\n",
       "      <th>R</th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>C</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>V</th>\n",
       "      <th>E</th>\n",
       "      <th>K</th>\n",
       "      <th>N</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>B</th>\n",
       "      <th>O</th>\n",
       "      <th>D</th>\n",
       "      <th>T</th>\n",
       "      <th>M</th>\n",
       "      <th>Q</th>\n",
       "      <th>H</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         U    F     R    P    S    C     A     G     V     E     K     N    J  \\\n",
       "U      1.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "F      0.0  1.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   1.0  0.0   \n",
       "R      0.0  0.0  25.0  0.0  0.0  0.0   1.0   1.0   0.0   8.0   4.0   1.0  0.0   \n",
       "P      0.0  0.0   0.0  1.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "S      0.0  0.0   0.0  0.0  2.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "C      0.0  0.0   0.0  0.0  0.0  2.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "A      0.0  0.0   0.0  0.0  0.0  4.0  66.0   0.0   0.0   3.0   2.0   1.0  0.0   \n",
       "G      0.0  0.0   0.0  0.0  0.0  0.0   0.0   1.0   0.0   0.0   0.0   0.0  0.0   \n",
       "V      0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0  19.0   3.0   0.0   0.0  0.0   \n",
       "E      0.0  0.0   1.0  0.0  0.0  0.0   4.0   3.0   3.0  10.0   2.0   3.0  0.0   \n",
       "K      0.0  0.0   2.0  0.0  3.0  1.0   4.0   2.0   0.0   5.0  15.0   1.0  0.0   \n",
       "N      0.0  0.0   0.0  0.0  0.0  0.0   1.0   1.0   0.0   3.0   0.0   9.0  0.0   \n",
       "J      0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  2.0   \n",
       "L      0.0  0.0   1.0  0.0  0.0  0.0   2.0   5.0   0.0   3.0   5.0   1.0  0.0   \n",
       "B      0.0  0.0   0.0  0.0  0.0  1.0   2.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "O      0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "D      0.0  0.0   0.0  0.0  1.0  0.0   2.0   0.0   2.0   0.0   0.0   1.0  0.0   \n",
       "T      0.0  0.0   0.0  0.0  0.0  0.0   0.0   1.0   0.0   0.0   0.0   0.0  0.0   \n",
       "M      0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "Q      0.0  0.0   0.0  0.0  0.0  1.0   2.0   0.0   0.0   2.0   0.0   5.0  0.0   \n",
       "H      0.0  0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0   2.0   0.0   1.0  0.0   \n",
       "total  1.0  1.0  29.0  1.0  6.0  9.0  84.0  14.0  24.0  39.0  28.0  24.0  2.0   \n",
       "\n",
       "          L     B    O     D    T    M     Q    H  total  \n",
       "U       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    1.0  \n",
       "F       0.0   0.0  0.0   0.0  0.0  1.0   0.0  0.0    3.0  \n",
       "R       1.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0   41.0  \n",
       "P       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    1.0  \n",
       "S       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    2.0  \n",
       "C       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    2.0  \n",
       "A       0.0   0.0  0.0   4.0  0.0  2.0   0.0  0.0   82.0  \n",
       "G       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    1.0  \n",
       "V       0.0   0.0  0.0   1.0  0.0  0.0   0.0  0.0   23.0  \n",
       "E       1.0   0.0  1.0   0.0  0.0  0.0   1.0  1.0   30.0  \n",
       "K       2.0   0.0  0.0   0.0  0.0  0.0   2.0  1.0   38.0  \n",
       "N       0.0   0.0  0.0   2.0  0.0  0.0   0.0  1.0   17.0  \n",
       "J       0.0   0.0  0.0   0.0  0.0  0.0   0.0  0.0    2.0  \n",
       "L      12.0   0.0  0.0   0.0  0.0  1.0   0.0  0.0   30.0  \n",
       "B       0.0  20.0  0.0   0.0  0.0  0.0   0.0  1.0   24.0  \n",
       "O       0.0   0.0  1.0   0.0  0.0  0.0   0.0  0.0    1.0  \n",
       "D       0.0   0.0  0.0   5.0  0.0  1.0   0.0  0.0   12.0  \n",
       "T       0.0   0.0  0.0   0.0  2.0  0.0   0.0  0.0    3.0  \n",
       "M       0.0   0.0  0.0   0.0  0.0  2.0   0.0  0.0    2.0  \n",
       "Q       0.0   0.0  0.0   2.0  0.0  0.0   7.0  0.0   19.0  \n",
       "H       0.0   0.0  0.0   0.0  0.0  0.0   0.0  2.0    5.0  \n",
       "total  16.0  20.0  2.0  14.0  2.0  7.0  10.0  6.0  339.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>histologic-type</th>\n",
       "      <th>degree-of-diffe</th>\n",
       "      <th>bone</th>\n",
       "      <th>bone-marrow</th>\n",
       "      <th>lung</th>\n",
       "      <th>pleura</th>\n",
       "      <th>peritoneum</th>\n",
       "      <th>liver</th>\n",
       "      <th>brain</th>\n",
       "      <th>skin</th>\n",
       "      <th>neck</th>\n",
       "      <th>supraclavicular</th>\n",
       "      <th>axillar</th>\n",
       "      <th>mediastinum</th>\n",
       "      <th>abdominal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.19764</td>\n",
       "      <td>0.457227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age      sex  histologic-type  degree-of-diffe  bone  bone-marrow  lung  \\\n",
       "0  0.0  0.00295          0.19764         0.457227   0.0          0.0   0.0   \n",
       "\n",
       "   pleura  peritoneum  liver  brain     skin  neck  supraclavicular  axillar  \\\n",
       "0     0.0         0.0    0.0    0.0  0.00295   0.0              0.0  0.00295   \n",
       "\n",
       "   mediastinum  abdominal  \n",
       "0          0.0        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_nb = SimpleNB(\"epsilon\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(make_confusion_matrix(\"primary-tumor.csv\"))\n",
    "    # display(make_confusion_matrix(\"cmc.csv\"))\n",
    "    X, y = preprocess(\"primary-tumor.csv\", split=True)\n",
    "    # display(df) \n",
    "    # Very high null rate col 2 and 3\n",
    "    display(show_null_portion(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can clearly see that the **Cardinality of label** clearly is much larger and thus resulting in a very **uninformative prior distribution** for the model to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score       0.522124\n",
       "0           0.154742\n",
       "1           0.321966\n",
       "2           0.212462\n",
       "3           0.020367\n",
       "4           0.100881\n",
       "5           0.067873\n",
       "6           0.220522\n",
       "7           0.199761\n",
       "8           0.067145\n",
       "9           0.053688\n",
       "10          0.291530\n",
       "11          0.127154\n",
       "12          0.240324\n",
       "13          0.184258\n",
       "14          0.170148\n",
       "max_ig      0.321966\n",
       "total_ig    2.432821\n",
       "avg_ig      0.162188\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try fit without those high null value columns and see info gain\n",
    "X_dropped = X.drop(columns=[\"histologic-type\", \"degree-of-diffe\"])\n",
    "simple_nb.fit(X_dropped, y)\n",
    "display(pd.Series(simple_nb.summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results: size of confusion matrix and the distribution of class, as well as the missing values. We see that the attribute that contributes most information gain has 45% of the values were missing and the distribution of classes are very sparse. With the majority label taking only less than 25% of all labels, the performance of 60% is not that bad. Actually more impressive than the breast-cancer case. We further the analysis by removing the attributes with most missing values, we then can see the performance dropped significantly.\n",
    "\n",
    "Thus based on the detailed analysis of those two cases. And other information in the first table, we conclude that higher information gain has a high correlation with high accuracy comparing to baseline models.\n",
    "\n",
    "In fact, I deliberatly hid the baseline models, here are all models with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>score</th>\n",
       "      <th>total_ig</th>\n",
       "      <th>max_ig</th>\n",
       "      <th>avg_ig</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anneal.csv</th>\n",
       "      <td>0.761693</td>\n",
       "      <td>0.991091</td>\n",
       "      <td>3.087582</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.088217</td>\n",
       "      <td>0.409090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306052</td>\n",
       "      <td>0.051344</td>\n",
       "      <td>0.291082</td>\n",
       "      <td>0.147119</td>\n",
       "      <td>0.213723</td>\n",
       "      <td>0.292235</td>\n",
       "      <td>0.126166</td>\n",
       "      <td>0.141074</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.038702</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117225</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>0.137181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04324</td>\n",
       "      <td>0.033038</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>2.293987e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast-cancer.csv</th>\n",
       "      <td>0.702797</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.306648</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.025819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.244755e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car.csv</th>\n",
       "      <td>0.700231</td>\n",
       "      <td>0.873843</td>\n",
       "      <td>0.686494</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.114416</td>\n",
       "      <td>0.096449</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.219663</td>\n",
       "      <td>0.030008</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.736111e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmc.csv</th>\n",
       "      <td>0.427020</td>\n",
       "      <td>0.505771</td>\n",
       "      <td>0.303960</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.037995</td>\n",
       "      <td>0.070906</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.875085e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis.csv</th>\n",
       "      <td>0.793548</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.696271</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.053559</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.083971</td>\n",
       "      <td>0.081544</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.109644</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.084933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.806452e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothyroid.csv</th>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.040715</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.883383e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mushroom.csv</th>\n",
       "      <td>0.517971</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>4.444996</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.202045</td>\n",
       "      <td>0.048797</td>\n",
       "      <td>0.028590</td>\n",
       "      <td>0.036049</td>\n",
       "      <td>0.192379</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>0.230154</td>\n",
       "      <td>0.416978</td>\n",
       "      <td>0.007517</td>\n",
       "      <td>0.191740</td>\n",
       "      <td>0.284726</td>\n",
       "      <td>0.271894</td>\n",
       "      <td>0.253845</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.038453</td>\n",
       "      <td>0.318022</td>\n",
       "      <td>0.480705</td>\n",
       "      <td>0.201958</td>\n",
       "      <td>0.156834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.791974e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery.csv</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.903086</td>\n",
       "      <td>1.291786</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.072935</td>\n",
       "      <td>0.196449</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.697531e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary-tumor.csv</th>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.604720</td>\n",
       "      <td>3.315571</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.195034</td>\n",
       "      <td>0.154742</td>\n",
       "      <td>0.321966</td>\n",
       "      <td>0.382902</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.212462</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.220522</td>\n",
       "      <td>0.199761</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.053688</td>\n",
       "      <td>0.291530</td>\n",
       "      <td>0.127154</td>\n",
       "      <td>0.240324</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>0.170148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.569322e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   baseline     score  total_ig    max_ig    avg_ig         0  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.761693  0.991091  3.087582  0.435178  0.088217  0.409090   \n",
       "breast-cancer.csv  0.702797  0.755245  0.306648  0.077010  0.034072  0.010606   \n",
       "car.csv            0.700231  0.873843  0.686494  0.262184  0.114416  0.096449   \n",
       "cmc.csv            0.427020  0.505771  0.303960  0.101740  0.037995  0.070906   \n",
       "hepatitis.csv      0.793548  0.851613  0.696271  0.132771  0.053559  0.036607   \n",
       "hypothyroid.csv    0.952261  0.952261  0.040715  0.009354  0.002262  0.000245   \n",
       "mushroom.csv       0.517971  0.997169  4.444996  0.906075  0.202045  0.048797   \n",
       "nursery.csv        0.333333  0.903086  1.291786  0.958775  0.161473  0.072935   \n",
       "primary-tumor.csv  0.247788  0.604720  3.315571  0.499848  0.195034  0.154742   \n",
       "\n",
       "                          1         2         3         4         5         6  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.000000  0.306052  0.051344  0.291082  0.147119  0.213723   \n",
       "breast-cancer.csv  0.002002  0.057171  0.068995  0.053458  0.077010  0.002489   \n",
       "car.csv            0.073704  0.004486  0.219663  0.030008  0.262184       NaN   \n",
       "cmc.csv            0.040139  0.101740  0.009821  0.002582  0.030474  0.032511   \n",
       "hepatitis.csv      0.013136  0.014491  0.083971  0.081544  0.012010  0.008603   \n",
       "hypothyroid.csv    0.000914  0.001238  0.000148  0.000999  0.001368  0.000542   \n",
       "mushroom.csv       0.028590  0.036049  0.192379  0.906075  0.014165  0.100883   \n",
       "nursery.csv        0.196449  0.005573  0.011886  0.019602  0.004333  0.022233   \n",
       "primary-tumor.csv  0.321966  0.382902  0.499848  0.212462  0.020367  0.100881   \n",
       "\n",
       "                          7         8         9        10        11        12  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.292235  0.126166  0.141074  0.032488  0.435178  0.038702   \n",
       "breast-cancer.csv  0.009099  0.025819       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv            0.015786       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv      0.002339  0.036702  0.109644  0.132771  0.079521  0.084933   \n",
       "hypothyroid.csv    0.000435  0.000489  0.000898  0.000045  0.000079  0.009354   \n",
       "mushroom.csv       0.230154  0.416978  0.007517  0.191740  0.284726  0.271894   \n",
       "nursery.csv        0.958775       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.067873  0.220522  0.199761  0.067145  0.053688  0.291530   \n",
       "\n",
       "                         13        14        15        16        17        18  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.000438  0.039356  0.021775  0.037997  0.036703  0.000000   \n",
       "breast-cancer.csv       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hypothyroid.csv    0.004075  0.005793  0.005768  0.005744  0.002580       NaN   \n",
       "mushroom.csv       0.253845  0.241416  0.000000  0.023817  0.038453  0.318022   \n",
       "nursery.csv             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.127154  0.240324  0.184258  0.170148       NaN       NaN   \n",
       "\n",
       "                         19        20        21   22        23        24   25  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.117225  0.029754  0.027042  0.0  0.015605  0.137181  0.0   \n",
       "breast-cancer.csv       NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "car.csv                 NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "cmc.csv                 NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "hepatitis.csv           NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "hypothyroid.csv         NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "mushroom.csv       0.480705  0.201958  0.156834  NaN       NaN       NaN  NaN   \n",
       "nursery.csv             NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "primary-tumor.csv       NaN       NaN       NaN  NaN       NaN       NaN  NaN   \n",
       "\n",
       "                         26        27   28   29   30       31        32  \\\n",
       "data                                                                      \n",
       "anneal.csv         0.022397  0.018242  0.0  0.0  0.0  0.04324  0.033038   \n",
       "breast-cancer.csv       NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "car.csv                 NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "cmc.csv                 NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "hypothyroid.csv         NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "mushroom.csv            NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "nursery.csv             NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "primary-tumor.csv       NaN       NaN  NaN  NaN  NaN      NaN       NaN   \n",
       "\n",
       "                         33        34    difference  \n",
       "data                                                 \n",
       "anneal.csv         0.019379  0.003959  2.293987e-01  \n",
       "breast-cancer.csv       NaN       NaN  5.244755e-02  \n",
       "car.csv                 NaN       NaN  1.736111e-01  \n",
       "cmc.csv                 NaN       NaN  7.875085e-02  \n",
       "hepatitis.csv           NaN       NaN  5.806452e-02  \n",
       "hypothyroid.csv         NaN       NaN  6.883383e-14  \n",
       "mushroom.csv            NaN       NaN  4.791974e-01  \n",
       "nursery.csv             NaN       NaN  5.697531e-01  \n",
       "primary-tumor.csv       NaN       NaN  3.569322e-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q1_table = q1_evaluation(split=0, smoothing=\"epsilon\", epsilon=1e-12, baseline=True)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(q1_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this diagram, it's very clear that **the more IG** dataset have, **the more accuracy compared to baseline model** the naive bayes model will have. Those ones with **low IG gained less than 20% accuracy** compared to blindly predicting the most common class. While the ones with higher IG will get much better performance boost **ranging fromg 20% to 49%**.\n",
    "\n",
    "---\n",
    "From this, it's also notable that the **nursery.csv**, despite not quite the high overall IG, its **high single attribute IG**, contributes a lot to the model. Making the performance getting a huge leap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5089621251879626 -0.025832695765089203\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Include a plot wrt to average gain as evidence\n",
    "difference = q1_table.loc[:, \"difference\"]\n",
    "n = len(difference)\n",
    "avg_ig = q1_table.loc[:, \"avg_ig\"]\n",
    "plt.plot(avg_ig, difference, \"ro\")\n",
    "# Compute OLE\n",
    "mean_avg_ig = sum(avg_ig) / n\n",
    "mean_squared_avg_ig = sum([i**2 for i in avg_ig]) / n\n",
    "mean_difference = sum(difference) / n\n",
    "mean_product = sum([difference[i] * avg_ig[i] for i in range(len(difference))]) / n\n",
    "\n",
    "slope = (mean_product - mean_avg_ig * mean_difference) / (mean_squared_avg_ig - mean_avg_ig ** 2)\n",
    "intercept =  mean_difference - slope * mean_avg_ig\n",
    "print(slope, intercept)\n",
    "x1, y1, x2, y2 = 0, intercept, 0.25, 0.25 * slope + intercept\n",
    "plt.plot([x1, x2], [y1, y2])\n",
    "plt.xlabel(\"Average information gain per attribute\")\n",
    "plt.ylabel(\"Accuracy difference (model - baseline)\")\n",
    "plt.show()\n",
    "\n",
    "# NOTE: This is not quite a linear model. More like a log normal model. I'm too lasy to transform.\n",
    "# NOTE: Such relations can also be seen in the second question, where it's shown as variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_ig</th>\n",
       "      <th>max_ig</th>\n",
       "      <th>avg_ig</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anneal.csv</th>\n",
       "      <td>0.989978</td>\n",
       "      <td>3.087582</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.088217</td>\n",
       "      <td>0.409090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306052</td>\n",
       "      <td>0.051344</td>\n",
       "      <td>0.291082</td>\n",
       "      <td>0.147119</td>\n",
       "      <td>0.213723</td>\n",
       "      <td>0.292235</td>\n",
       "      <td>0.126166</td>\n",
       "      <td>0.141074</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.435178</td>\n",
       "      <td>0.038702</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117225</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015605</td>\n",
       "      <td>0.137181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04324</td>\n",
       "      <td>0.033038</td>\n",
       "      <td>0.019379</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast-cancer.csv</th>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.306648</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.025819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car.csv</th>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.686494</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.114416</td>\n",
       "      <td>0.096449</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.219663</td>\n",
       "      <td>0.030008</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmc.csv</th>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.303960</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.037995</td>\n",
       "      <td>0.070906</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>0.101740</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis.csv</th>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.696271</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.053559</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.083971</td>\n",
       "      <td>0.081544</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.109644</td>\n",
       "      <td>0.132771</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.084933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothyroid.csv</th>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.040715</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mushroom.csv</th>\n",
       "      <td>0.997169</td>\n",
       "      <td>4.444996</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.202045</td>\n",
       "      <td>0.048797</td>\n",
       "      <td>0.028590</td>\n",
       "      <td>0.036049</td>\n",
       "      <td>0.192379</td>\n",
       "      <td>0.906075</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>0.230154</td>\n",
       "      <td>0.416978</td>\n",
       "      <td>0.007517</td>\n",
       "      <td>0.191740</td>\n",
       "      <td>0.284726</td>\n",
       "      <td>0.271894</td>\n",
       "      <td>0.253845</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.038453</td>\n",
       "      <td>0.318022</td>\n",
       "      <td>0.480705</td>\n",
       "      <td>0.201958</td>\n",
       "      <td>0.156834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery.csv</th>\n",
       "      <td>0.902855</td>\n",
       "      <td>1.291786</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.072935</td>\n",
       "      <td>0.196449</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.011886</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.022233</td>\n",
       "      <td>0.958775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary-tumor.csv</th>\n",
       "      <td>0.557522</td>\n",
       "      <td>3.315571</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.195034</td>\n",
       "      <td>0.154742</td>\n",
       "      <td>0.321966</td>\n",
       "      <td>0.382902</td>\n",
       "      <td>0.499848</td>\n",
       "      <td>0.212462</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.220522</td>\n",
       "      <td>0.199761</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.053688</td>\n",
       "      <td>0.291530</td>\n",
       "      <td>0.127154</td>\n",
       "      <td>0.240324</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>0.170148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score  total_ig    max_ig    avg_ig         0         1  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.989978  3.087582  0.435178  0.088217  0.409090  0.000000   \n",
       "breast-cancer.csv  0.741259  0.306648  0.077010  0.034072  0.010606  0.002002   \n",
       "car.csv            0.856481  0.686494  0.262184  0.114416  0.096449  0.073704   \n",
       "cmc.csv            0.504413  0.303960  0.101740  0.037995  0.070906  0.040139   \n",
       "hepatitis.csv      0.870968  0.696271  0.132771  0.053559  0.036607  0.013136   \n",
       "hypothyroid.csv    0.952261  0.040715  0.009354  0.002262  0.000245  0.000914   \n",
       "mushroom.csv       0.997169  4.444996  0.906075  0.202045  0.048797  0.028590   \n",
       "nursery.csv        0.902855  1.291786  0.958775  0.161473  0.072935  0.196449   \n",
       "primary-tumor.csv  0.557522  3.315571  0.499848  0.195034  0.154742  0.321966   \n",
       "\n",
       "                          2         3         4         5         6         7  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.306052  0.051344  0.291082  0.147119  0.213723  0.292235   \n",
       "breast-cancer.csv  0.057171  0.068995  0.053458  0.077010  0.002489  0.009099   \n",
       "car.csv            0.004486  0.219663  0.030008  0.262184       NaN       NaN   \n",
       "cmc.csv            0.101740  0.009821  0.002582  0.030474  0.032511  0.015786   \n",
       "hepatitis.csv      0.014491  0.083971  0.081544  0.012010  0.008603  0.002339   \n",
       "hypothyroid.csv    0.001238  0.000148  0.000999  0.001368  0.000542  0.000435   \n",
       "mushroom.csv       0.036049  0.192379  0.906075  0.014165  0.100883  0.230154   \n",
       "nursery.csv        0.005573  0.011886  0.019602  0.004333  0.022233  0.958775   \n",
       "primary-tumor.csv  0.382902  0.499848  0.212462  0.020367  0.100881  0.067873   \n",
       "\n",
       "                          8         9        10        11        12        13  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.126166  0.141074  0.032488  0.435178  0.038702  0.000438   \n",
       "breast-cancer.csv  0.025819       NaN       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv      0.036702  0.109644  0.132771  0.079521  0.084933       NaN   \n",
       "hypothyroid.csv    0.000489  0.000898  0.000045  0.000079  0.009354  0.004075   \n",
       "mushroom.csv       0.416978  0.007517  0.191740  0.284726  0.271894  0.253845   \n",
       "nursery.csv             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.220522  0.199761  0.067145  0.053688  0.291530  0.127154   \n",
       "\n",
       "                         14        15        16        17        18        19  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.039356  0.021775  0.037997  0.036703  0.000000  0.117225   \n",
       "breast-cancer.csv       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "car.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "cmc.csv                 NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "hypothyroid.csv    0.005793  0.005768  0.005744  0.002580       NaN       NaN   \n",
       "mushroom.csv       0.241416  0.000000  0.023817  0.038453  0.318022  0.480705   \n",
       "nursery.csv             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "primary-tumor.csv  0.240324  0.184258  0.170148       NaN       NaN       NaN   \n",
       "\n",
       "                         20        21   22        23        24   25        26  \\\n",
       "data                                                                            \n",
       "anneal.csv         0.029754  0.027042  0.0  0.015605  0.137181  0.0  0.022397   \n",
       "breast-cancer.csv       NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "car.csv                 NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "cmc.csv                 NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "hepatitis.csv           NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "hypothyroid.csv         NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "mushroom.csv       0.201958  0.156834  NaN       NaN       NaN  NaN       NaN   \n",
       "nursery.csv             NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "primary-tumor.csv       NaN       NaN  NaN       NaN       NaN  NaN       NaN   \n",
       "\n",
       "                         27   28   29   30       31        32        33  \\\n",
       "data                                                                      \n",
       "anneal.csv         0.018242  0.0  0.0  0.0  0.04324  0.033038  0.019379   \n",
       "breast-cancer.csv       NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "car.csv                 NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "cmc.csv                 NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "hepatitis.csv           NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "hypothyroid.csv         NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "mushroom.csv            NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "nursery.csv             NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "primary-tumor.csv       NaN  NaN  NaN  NaN      NaN       NaN       NaN   \n",
       "\n",
       "                         34  \n",
       "data                         \n",
       "anneal.csv         0.003959  \n",
       "breast-cancer.csv       NaN  \n",
       "car.csv                 NaN  \n",
       "cmc.csv                 NaN  \n",
       "hepatitis.csv           NaN  \n",
       "hypothyroid.csv         NaN  \n",
       "mushroom.csv            NaN  \n",
       "nursery.csv             NaN  \n",
       "primary-tumor.csv       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>avg_acc</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anneal.csv</th>\n",
       "      <td>0.989978</td>\n",
       "      <td>0.991111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast-cancer.csv</th>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car.csv</th>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.858960</td>\n",
       "      <td>0.861272</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.867052</td>\n",
       "      <td>0.803468</td>\n",
       "      <td>0.843931</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.832370</td>\n",
       "      <td>0.855491</td>\n",
       "      <td>0.890173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmc.csv</th>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.491892</td>\n",
       "      <td>0.425676</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.567568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis.csv</th>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothyroid.csv</th>\n",
       "      <td>0.952261</td>\n",
       "      <td>0.952366</td>\n",
       "      <td>0.946372</td>\n",
       "      <td>0.955836</td>\n",
       "      <td>0.940063</td>\n",
       "      <td>0.962145</td>\n",
       "      <td>0.952681</td>\n",
       "      <td>0.952681</td>\n",
       "      <td>0.958991</td>\n",
       "      <td>0.946372</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.943218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mushroom.csv</th>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.996925</td>\n",
       "      <td>0.995080</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.997540</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.997540</td>\n",
       "      <td>0.998770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery.csv</th>\n",
       "      <td>0.902855</td>\n",
       "      <td>0.903009</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.906636</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.908951</td>\n",
       "      <td>0.909722</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.902006</td>\n",
       "      <td>0.904321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary-tumor.csv</th>\n",
       "      <td>0.557522</td>\n",
       "      <td>0.532353</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   train_acc   avg_acc         1         2         3  \\\n",
       "data                                                                   \n",
       "anneal.csv          0.989978  0.991111  1.000000  0.977778  1.000000   \n",
       "breast-cancer.csv   0.741259  0.741379  0.689655  0.586207  0.758621   \n",
       "car.csv             0.856481  0.858960  0.861272  0.872832  0.867052   \n",
       "cmc.csv             0.504413  0.491892  0.425676  0.445946  0.500000   \n",
       "hepatitis.csv       0.870968  0.850000  0.875000  0.875000  0.812500   \n",
       "hypothyroid.csv     0.952261  0.952366  0.946372  0.955836  0.940063   \n",
       "mushroom.csv        0.997169  0.996925  0.995080  0.996310  0.996310   \n",
       "nursery.csv         0.902855  0.903009  0.907407  0.906636  0.901235   \n",
       "primary-tumor.csv   0.557522  0.532353  0.441176  0.647059  0.500000   \n",
       "\n",
       "                          4         5         6         7         8         9  \\\n",
       "data                                                                            \n",
       "anneal.csv         1.000000  0.977778  1.000000  1.000000  0.977778  0.988889   \n",
       "breast-cancer.csv  0.724138  0.793103  0.827586  0.827586  0.758621  0.689655   \n",
       "car.csv            0.803468  0.843931  0.872832  0.890173  0.832370  0.855491   \n",
       "cmc.csv            0.479730  0.500000  0.493243  0.391892  0.533784  0.581081   \n",
       "hepatitis.csv      0.875000  0.937500  0.625000  0.750000  1.000000  0.875000   \n",
       "hypothyroid.csv    0.962145  0.952681  0.952681  0.958991  0.946372  0.965300   \n",
       "mushroom.csv       0.998770  0.997540  0.991390  0.998770  0.998770  0.997540   \n",
       "nursery.csv        0.901235  0.890432  0.908951  0.909722  0.898148  0.902006   \n",
       "primary-tumor.csv  0.558824  0.500000  0.470588  0.617647  0.470588  0.617647   \n",
       "\n",
       "                         10  \n",
       "data                         \n",
       "anneal.csv         0.988889  \n",
       "breast-cancer.csv  0.758621  \n",
       "car.csv            0.890173  \n",
       "cmc.csv            0.567568  \n",
       "hepatitis.csv      0.875000  \n",
       "hypothyroid.csv    0.943218  \n",
       "mushroom.csv       0.998770  \n",
       "nursery.csv        0.904321  \n",
       "primary-tumor.csv  0.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_val_df(k=10):\n",
    "    fold = k\n",
    "    df = pd.DataFrame(columns=[\"data\"] + [\"train_acc\"] + [\"avg_acc\"] + list(range(1, k+1)))\n",
    "    \n",
    "    # Use *k-smoothing* with k being 1e-6\n",
    "    simple_nb = SimpleNB(smoothing=1e-6)\n",
    "    \n",
    "    # Use accuracy as our metrics\n",
    "    accuracy_scorer = AccuracyScorer()\n",
    "\n",
    "    # Use k-fold-validation\n",
    "    k_fold_validation = KFoldValidation(simple_nb, accuracy_scorer, k)\n",
    "    \n",
    "    for num, data_file in enumerate(data_files):        \n",
    "        row_dict = {}\n",
    "        \n",
    "        X, y = preprocess(data_file, split=True)\n",
    "        \n",
    "        if k == -1:\n",
    "            fold = X.shape[0]\n",
    "            k_fold_validation = KFoldValidation(simple_nb, accuracy_scorer, fold)\n",
    "        \n",
    "        train(X, y, simple_nb)\n",
    "        train_acc = evaluate(simple_nb.predict(X), y, scorer=accuracy_scorer)\n",
    "        row_dict[\"train_acc\"] = train_acc\n",
    "        \n",
    "        accuracies = k_fold_validation.fit_transform(X, y, shuf=True, seed=1000)\n",
    "        avg_acc = sum(accuracies) / k\n",
    "        \n",
    "        for i, j in enumerate(accuracies):\n",
    "            row_dict[i+1] = j\n",
    "        row_dict[\"data\"] = data_file\n",
    "        row_dict[\"avg_acc\"] = avg_acc\n",
    "        \n",
    "        df = df.append(row_dict, ignore_index=True)\n",
    "    df = df.set_index(\"data\")\n",
    "    return df\n",
    "\n",
    "q1_table = q1_evaluation(smoothing=1e-6)\n",
    "cross_val_table = cross_val_df(10)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(q1_table)\n",
    "    display(cross_val_df(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In theory as more data is available, the more information is available** to the model. Thus the **accuracy should increase**.\n",
    "\n",
    "---\n",
    "From the above two tables, we can see except for the last **primary-tumor.csv** has a **big accuracy drop**. For all others datasets, the performance only dropped by **insignificant amount or for some, it even increased**. As naive bayes's predicting power depends totally on the number of attributes and quality of attributes. The reason why this unexpected change in accuracy can be accounted to **random noise** which KFoldValidation can successfully reduce (as it's averaging over sets of estimations which will reduce the variance of particular distribution).\n",
    "\n",
    "This also might indicates that the dataset instances are not independent and some sort of noise correlation got filtered out while the predicting power of naive bayes is retained when training with KFoldValidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To verify either of these assumptions, let's try plot the errors\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=[12, 12])\n",
    "plt.boxplot(cross_val_table.iloc[:, 2:])\n",
    "i = 0\n",
    "for index, data in cross_val_table.iterrows():\n",
    "    i += 1\n",
    "    # The full training estimation\n",
    "    plt.plot(i, data.iloc[0], \"ro\")\n",
    "    # The average cross validation\n",
    "    plt.plot(i, data.iloc[1], \"bo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, \n",
    "- the red dot denotes the accuracy when use the whole set to train and test.\n",
    "- the blue dot denotes the accuracy of average KFoldValidatin\n",
    "---\n",
    "1. After this plot, we can clearly see that the performance is generally **worse when using KFoldValidation**. Especially when the **deivation of individual fits of KFoldValidation is high**. Meaning it's harder to get the prediction right. Thus the training for testing dataset will have lots of advantage in predicting the true label, as it has the *True underlying distribution of the data*. While the accuracy cross validation converges to is more of an accurate representation of the accuracy to be expected. As the **intrinsic variation in the dataset is very high**. The effect of **random noise will be diluted** by the **high model variance**, resulting more **training for test accuracy a lot higher than cross validation**.\n",
    "2. The above points can be made clearer by viewing those datasets with High accuracy and low variance for cross validations. Those datasets which have these perks and has higher accuracy than traning accuracy generally are **easier to predict**. As there is **less intrinsic variation of the model**, the effect of **random noise** are made more significant for these datasets. Making the **mean accuracy of cross validation higher** than train for test accuracy. And we can see the **cross validation only win by some insignificant magins** compared to the other case where **training set wins by a larger margin**.\n",
    "---\n",
    "So there is no suprising result with cross validation. We can say the effectiveness or **mean accuracy of cross validation will be lower** in most cases where the data is harder to predict and has more intrinsic variation. While it's **possible that the cross validation accuracy be higher**, but that's the **effect of some random noise** of the data and selection of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
