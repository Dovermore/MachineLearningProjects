{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_loading.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file contains models built only with the chisq data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data loading pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "class DataLoader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type, content=\"top10\"):\n",
    "        self._type = type\n",
    "        self._content = content\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return load_data_file(type=self._type, content=self._content)\n",
    "\n",
    "\n",
    "# For Transforming Locations to Labels\n",
    "class DataFrameLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, label=\"Location\", label_encoder=None):\n",
    "        self._label = label\n",
    "        if label_encoder is None:\n",
    "            self._label_encoder = LabelEncoder()\n",
    "        else:\n",
    "            self._label_encoder = label_encoder\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self._label_encoder.fit(X[self._label])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.assign(**{\"EncodedLabel\": self._label_encoder.transform(X[self._label])})\n",
    "\n",
    "\n",
    "# For splitting the dataset to X and y, without shuffling\n",
    "class ChisqDataAttributeLabel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, label=\"EncodedLabel\", columns_to_remove=[\"Instance_ID\", \"Location\"], shuffle=False):\n",
    "        self._columns_to_remove = columns_to_remove\n",
    "        self._label = label\n",
    "        self._shuffle = shuffle\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self._shuffle:\n",
    "            X = X.sample(frac=1)\n",
    "        X_train = X.loc[:, [i for i in X.columns if i not in self._columns_to_remove + [self._label]]].to_numpy()\n",
    "        y_train = X.loc[:, self._label].to_numpy()\n",
    "        return X_train, y_train\n",
    "    \n",
    "    \n",
    "def full_data_loading_pipeline(type=\"train\", content=\"top10\", label=\"Location\", transformed_label=\"EncodedLabel\", \n",
    "                               columns_to_remove=[\"Instance_ID\", \"Location\"], \n",
    "                               label_encoder=None, shuffle=False):\n",
    "    return Pipeline( \n",
    "        [\n",
    "        (\"DataLoader\", DataLoader(type=type, content=content)),\n",
    "        (\"LabelEncoder\", DataFrameLabelEncoder(label)),\n",
    "        (\"AttributeLabelProcessor\", ChisqDataAttributeLabel(transformed_label, shuffle=shuffle)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data  = {\n",
    "    content: {\n",
    "        type: full_data_loading_pipeline(type, content, shuffle=False).fit_transform(None) for type in [\"train\", \"dev\", \"test\"] \n",
    "    } for content in [\"top10\", \"top50\", \"top100\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTINOMINAL NB\n",
    "From our prior observation, this might not perform well for the prior for this dataset is very uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10: 0.29491370993675636\n",
      "top50: 0.30134526744559975\n",
      "top100: 0.3078304212670168\n"
     ]
    }
   ],
   "source": [
    "nb_accs = []\n",
    "for content in all_data:\n",
    "    mnb = MultinomialNB()\n",
    "    X_train, y_train = all_data[content][\"train\"]\n",
    "    X_dev, y_dev = all_data[content][\"dev\"]\n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_dev, y_dev)\n",
    "    nb_accs.append((mnb, acc))\n",
    "    print(f\"{content}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import Nystroem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10: 0.2936541965912745\n",
      "top50: 0.29553006753135386\n",
      "top100: 0.3061421374209454\n"
     ]
    }
   ],
   "source": [
    "sgd_svc_accs = []\n",
    "for content in all_data:\n",
    "    svc = SGDClassifier(max_iter=10000, random_state=42)\n",
    "    nys = Nystroem(gamma=.2)\n",
    "    X_train = nys.fit_transform(X_train)\n",
    "    X_train, y_train = all_data[content][\"train\"]\n",
    "    X_dev, y_dev = all_data[content][\"dev\"]\n",
    "    svc.fit(X_train, y_train)\n",
    "    acc = svc.score(X_dev, y_dev)\n",
    "    sgd_svc_accs.append((svc, acc))\n",
    "    print(f\"{content}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10: 0.2954764712187801\n",
      "top50: 0.2982366813163254\n",
      "top100: 0.30381069782398973\n"
     ]
    }
   ],
   "source": [
    "rfc_accs = []\n",
    "for content in all_data:\n",
    "    rfc = RandomForestClassifier(n_estimators=20)\n",
    "    X_train, y_train = all_data[content][\"train\"]\n",
    "    X_dev, y_dev = all_data[content][\"dev\"]\n",
    "    rfc.fit(X_train, y_train)\n",
    "    acc = rfc.score(X_dev, y_dev)\n",
    "    rfc_accs.append((rfc, acc))\n",
    "    print(f\"{content}: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10: 0.2954764712187801\n",
      "top50: 0.3000321577875442\n",
      "top100: 0.3071872655161325\n"
     ]
    }
   ],
   "source": [
    "etc_accs = []\n",
    "for content in all_data:\n",
    "    etc = ExtraTreesClassifier(n_estimators=20)\n",
    "    X_train, y_train = all_data[content][\"train\"]\n",
    "    X_dev, y_dev = all_data[content][\"dev\"]\n",
    "    etc.fit(X_train, y_train)\n",
    "    acc = etc.score(X_dev, y_dev)\n",
    "    etc_accs.append((etc, acc))\n",
    "    print(f\"{content}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Lots of others with a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, log_loss\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from time import process_time\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "\n",
    "def rank_models(classifiers, X, y, n_splits=10):\n",
    "    log_cols_acc = [\"Classifier\", \"Accuracy\"]\n",
    "    log_cols_prec = [\"Classifier\", \"Precision\"]\n",
    "    log_cols_rec = [\"Classifier\", \"Recall\"]\n",
    "    log_cols_f1 = [\"Classifier\", \"F1\"]\n",
    "    log_acc = pd.DataFrame(columns=log_cols_acc)\n",
    "    log_prec = pd.DataFrame(columns=log_cols_prec)\n",
    "    log_rec = pd.DataFrame(columns=log_cols_rec)\n",
    "    log_f1 = pd.DataFrame(columns=log_cols_f1)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.05, random_state=42)\n",
    "    \n",
    "    fitting_time_dict = dd(float)\n",
    "    prec_dict = dd(float)\n",
    "    rec_dict = dd(float)\n",
    "    acc_dict = dd(float)\n",
    "    f1_dict = dd(float)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        print(f\"==================== Split{i} ====================\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        for clf in classifiers:\n",
    "            name = clf.__class__.__name__\n",
    "            print(f\"classifier {name}\")\n",
    "            start = process_time()\n",
    "            clf.fit(X_train, y_train)\n",
    "            duration = process_time() - start\n",
    "            train_predictions = clf.predict(X_test)\n",
    "            acc = accuracy_score(y_test, train_predictions)\n",
    "            prec = precision_score(y_test, train_predictions, average=\"micro\")\n",
    "            rec = recall_score(y_test, train_predictions, average=\"micro\")\n",
    "            f1 = f1_score(y_test, train_predictions, average=\"micro\")\n",
    "\n",
    "            fitting_time_dict[name] += duration\n",
    "            acc_dict[name] += acc\n",
    "            prec_dict[name] += prec\n",
    "            rec_dict[name] += rec\n",
    "            f1_dict[name] += f1\n",
    "\n",
    "    for clf in acc_dict:\n",
    "        acc_dict[clf] = acc_dict[clf] / n_splits\n",
    "        log_entry_acc = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols_acc)\n",
    "        log_acc = log_acc.append(log_entry_acc)\n",
    "\n",
    "    for clf in prec_dict:\n",
    "        prec_dict[clf] = prec_dict[clf] / n_splits\n",
    "        log_entry_prec = pd.DataFrame([[clf, prec_dict[clf]]], columns=log_cols_prec)\n",
    "        log_prec = log_prec.append(log_entry_prec)\n",
    "\n",
    "    for clf in rec_dict:\n",
    "        rec_dict[clf] = rec_dict[clf] / n_splits\n",
    "        log_entry_rec = pd.DataFrame([[clf, rec_dict[clf]]], columns=log_cols_rec)\n",
    "        log_rec = log_rec.append(log_entry_rec)\n",
    "\n",
    "    for clf in f1_dict:\n",
    "        f1_dict[clf] = f1_dict[clf] / n_splits\n",
    "        log_entry_f1 = pd.DataFrame([[clf, f1_dict[clf]]], columns=log_cols_f1)\n",
    "        log_f1 = log_f1.append(log_entry_f1)\n",
    "\n",
    "    for clf in f1_dict:\n",
    "        f1_dict[clf] = f1_dict[clf] / n_splits\n",
    "        log_entry_f1 = pd.DataFrame([[clf, f1_dict[clf]]], columns=log_cols_f1)\n",
    "        log_f1 = log_f1.append(log_entry_f1)\n",
    "\n",
    "    return fitting_time_dict, acc_dict, log_acc, prec_dict, log_prec, rec_dict, log_rec, f1_dict, log_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Split0 ====================\n",
      "classifier SVC\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = all_data[\"top10\"][\"train\"]\n",
    "X_dev, y_dev = all_data[\"top10\"][\"dev\"]\n",
    "X = np.append(X_train, X_dev, axis=0)\n",
    "y = np.append(y_train, y_dev)\n",
    "\n",
    "classifiers = [\n",
    "    SVC(gamma=\"scale\"),\n",
    "    SGDClassifier(),\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=20),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\", max_iter=10000)\n",
    "]\n",
    "\n",
    "\n",
    "size=10000000\n",
    "fitting_time_dict, acc_dict, log_acc, prec_dict, log_prec, rec_dict, log_rec, f1_dict, log_f1 = \\\n",
    "            rank_models(classifiers, X[:size], y[:size], n_splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log_acc, color=\"b\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Precision')\n",
    "plt.title('Classifier Precision')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Precision', y='Classifier', data=log_prec, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Recall')\n",
    "plt.title('Classifier Recall')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Recall', y='Classifier', data=log_rec, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('F1')\n",
    "plt.title('Classifier F1 Score')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='F1', y='Classifier', data=log_f1, color=\"b\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DecisionTreeClassifier', 0.292),\n",
       " ('GradientBoostingClassifier', 0.292),\n",
       " ('RandomForestClassifier', 0.288),\n",
       " ('LogisticRegression', 0.288),\n",
       " ('AdaBoostClassifier', 0.2866666666666667),\n",
       " ('SVC', 0.2853333333333333),\n",
       " ('SGDClassifier', 0.2853333333333333),\n",
       " ('KNeighborsClassifier', 0.2813333333333334),\n",
       " ('GaussianNB', 0.27466666666666667)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(acc_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DecisionTreeClassifier', 0.09733333333333333),\n",
       " ('GradientBoostingClassifier', 0.09733333333333333),\n",
       " ('RandomForestClassifier', 0.09599999999999999),\n",
       " ('LogisticRegression', 0.09599999999999999),\n",
       " ('AdaBoostClassifier', 0.09555555555555556),\n",
       " ('SVC', 0.0951111111111111),\n",
       " ('SGDClassifier', 0.0951111111111111),\n",
       " ('KNeighborsClassifier', 0.0937777777777778),\n",
       " ('GaussianNB', 0.09155555555555556)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(f1_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try tune logistic regression, GradientBoosting and GaussianNB\n",
    "(SVC will take too long to tune. We will leave it till the last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also need test elastic net!\n",
    "lr_param_dict = {\n",
    "#     \"penalty\": [\"l1\", \"l2\", \"none\"],\n",
    "    \"penalty\": [\"l2\", \"none\"],\n",
    "    \"C\": np.logspace(-3,3, 14),\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"max_iter\": [10000],\n",
    "    \"multi_class\": [\"ovr\", \"multinomial\"],\n",
    "#     \"multi_class\": [\"ovr\"],\n",
    "    \"solver\": [\"lbfgs\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance_ID</th>\n",
       "      <th>abcbrisbane</th>\n",
       "      <th>abotlangit</th>\n",
       "      <th>advanceqld</th>\n",
       "      <th>afcuw</th>\n",
       "      <th>afdonnerwetter</th>\n",
       "      <th>afl</th>\n",
       "      <th>afleaglesfreo</th>\n",
       "      <th>aflfantasy</th>\n",
       "      <th>aflpieseagles</th>\n",
       "      <th>...</th>\n",
       "      <th>victraffic</th>\n",
       "      <th>voodoo</th>\n",
       "      <th>wa</th>\n",
       "      <th>waterpoloa</th>\n",
       "      <th>waterpoloaus</th>\n",
       "      <th>waterpolosa</th>\n",
       "      <th>western</th>\n",
       "      <th>wests</th>\n",
       "      <th>xx</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance_ID  abcbrisbane  abotlangit  advanceqld  afcuw  afdonnerwetter  \\\n",
       "0           11            0           0           0      0               0   \n",
       "1           12            0           0           0      0               0   \n",
       "2           13            0           0           0      0               0   \n",
       "3           14            0           0           0      0               0   \n",
       "4           15            0           0           0      0               0   \n",
       "\n",
       "   afl  afleaglesfreo  aflfantasy  aflpieseagles  ...  victraffic  voodoo  wa  \\\n",
       "0    0              0           0              0  ...           0       0   0   \n",
       "1    0              0           0              0  ...           0       0   0   \n",
       "2    0              0           0              0  ...           0       0   0   \n",
       "3    0              0           0              0  ...           0       0   0   \n",
       "4    0              0           0              0  ...           0       0   0   \n",
       "\n",
       "   waterpoloa  waterpoloaus  waterpolosa  western  wests  xx   Location  \n",
       "0           0             0            0        0      0   0  Melbourne  \n",
       "1           0             0            0        0      0   0  Melbourne  \n",
       "2           0             0            0        0      0   0   Brisbane  \n",
       "3           0             0            0        0      0   0      Perth  \n",
       "4           0             0            0        0      0   0      Perth  \n",
       "\n",
       "[5 rows x 187 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_file(\"train\", \"top50\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_search_cv = GridSearchCV(estimator=LogisticRegression(), param_grid=lr_param_dict, \n",
    "                            return_train_score=True, n_jobs=-1, verbose=3, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/mlp2/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 336 out of 336 | elapsed:  5.2min finished\n",
      "/anaconda/envs/mlp2/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([1.0000...\n",
       "       7.01703829e-02, 2.03091762e-01, 5.87801607e-01, 1.70125428e+00,\n",
       "       4.92388263e+00, 1.42510267e+01, 4.12462638e+01, 1.19377664e+02,\n",
       "       3.45510729e+02, 1.00000000e+03]),\n",
       "                         'fit_intercept': [True, False], 'max_iter': [10000],\n",
       "                         'multi_class': ['ovr', 'multinomial'],\n",
       "                         'penalty': ['l2', 'none'], 'solver': ['lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.append(X_train, X_dev, axis=0)\n",
    "y = np.append(y_train, y_dev)\n",
    "lr_search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29680023923444976,\n",
       " LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=False,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                    multi_class='ovr', n_jobs=None, penalty='none',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_search_cv.best_score_, lr_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
